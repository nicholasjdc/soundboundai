<!DOCTYPE html>
<html>
  <head>
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Hello!</title>
    <meta name="description" content="description"/>
    <meta name="author" content="author" />
    <meta name="keywords" content="keywords" />
    <link rel="stylesheet" href="./stylesheet.css" type="text/css" />
    <style type="text/css">.body { width: auto; }</style>
  </head>
  <body>
    </p>
    <a href="timeline.pdf">Download PDF</a>
    <div id="image">
        <p style="font-size: 80%;">This graph is clickable!</p>
        <img src="timeline.PNG"  width = 550 height = 350 alt="Timeline image" usemap="#timeline" />
    </div>
    <h2>Details</h2>
    <map id="timeline" name="timeline">
    <area shape="rect" id="node1" href="#MusicLM" title="MusicLM" alt="" coords="175,0,240,60"/>
    <area shape="rect" id="node2" href="#AudioLM" title="AudioLM" alt="" coords="175,80,240,110"/>
    <area shape="rect" id="node3" href="#MuLan" title="MuLan" alt="" coords="430,110,490,150"/>
    <area shape="rect" id="node4" href="#wav2vec-bert" title="wav2vec-BERT" alt="" coords="60,160,190,190"/>
    <area shape="rect" id="node5" href="#SoundStream" title="SoundStream" alt="" coords="300,200,400,230"/>
    <area shape="rect" id="node6" href="#wav2vec" title="wav2Vec" alt="" coords="60,250,150,280"/>
    <area shape="rect" id="node7" href="#BERT" title="BERT" alt="" coords="190,290,310,330"/>

    </map>
    <section id="MusicLM">
        <h3>MusicLM</h3>
        <dt>Paper name</dt><dd>Alpaca: A Strong, Replicable Instruction-Following Model</dd>
        <dt>Paper authors</dt><dd>Taori et al.</dd>
        <dt>Paper link</dt><dd><a href="https://crfm.stanford.edu/2023/03/13/alpaca.html">https://crfm.stanford.edu/2023/03/13/alpaca.html</a></dd>
        <dt>Publication Date</dt><dd>January 2023</dd>
        <dt>Repository link</dt><dd><a href="https://github.com/tatsu-lab/stanford_alpaca">https://github.com/tatsu-lab/stanford_alpaca</a></dd>
        <dt>Affiliation</dt><dd>Stanford</dd>
        <dt>Great Secondary Resources</dt><dd>SOMETHING</dd>
        <a href="#">&uarr; Back to top</a>
        </section>
    <section id="AudioLM">
        <h3>AudioLM</h3>
        <dt>Paper name</dt><dd>Alpaca: A Strong, Replicable Instruction-Following Model</dd>
        <dt>Paper authors</dt><dd>Taori et al.</dd>
        <dt>Paper link</dt><dd><a href="https://crfm.stanford.edu/2023/03/13/alpaca.html">https://crfm.stanford.edu/2023/03/13/alpaca.html</a></dd>
        <dt>Publication Date</dt><dd>September 2022</dd>
        <dt>Repository link</dt><dd><a href="https://github.com/tatsu-lab/stanford_alpaca">https://github.com/tatsu-lab/stanford_alpaca</a></dd>
        <dt>Affiliation</dt><dd>Stanford</dd>
        <dt>Great Secondary Resources</dt><dd>SOMETHING</dd>

        <a href="#">&uarr; Back to top</a>
        </section>
    <section id="MuLan">
        <h3>MuLan</h3>
        <dt>Paper name</dt><dd>Alpaca: A Strong, Replicable Instruction-Following Model</dd>
        <dt>Paper authors</dt><dd>Taori et al.</dd>
        <dt>Paper link</dt><dd><a href="https://crfm.stanford.edu/2023/03/13/alpaca.html">https://crfm.stanford.edu/2023/03/13/alpaca.html</a></dd>
        <dt>Publication Date</dt><dd>August 2022</dd>
        <dt>Repository link</dt><dd><a href="https://github.com/tatsu-lab/stanford_alpaca">https://github.com/tatsu-lab/stanford_alpaca</a></dd>
        <dt>Affiliation</dt><dd>Stanford</dd>
        <dt>Great Secondary Resources</dt><dd>SOMETHING</dd>

        <a href="#">&uarr; Back to top</a>
        </section>
    <section id="wav2vec-bert">
        <h3>Wav2Vec-BERT</h3>
        <dt>Paper name</dt><dd>W2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training</dd>
        <dt>Paper authors</dt><dd>Taori et al.</dd>
        <dt>Paper link</dt><dd><a href="https://arxiv.org/abs/2108.06209">https://arxiv.org/abs/2108.06209</a></dd>
        <dt>Publication Date</dt><dd>August 2021</dd>
        <dt>Repository link</dt><dd><a href="https://github.com/tatsu-lab/stanford_alpaca">https://github.com/tatsu-lab/stanford_alpaca</a></dd>
        <dt>Affiliation</dt><dd>Stanford</dd>
        <dt>Great Secondary Resources</dt><dd>SOMETHING</dd>
        <dt>Bib Tex Citation</dt><dd>@misc{chung2021w2vbert,
            title={W2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training}, 
            author={Yu-An Chung and Yu Zhang and Wei Han and Chung-Cheng Chiu and James Qin and Ruoming Pang and Yonghui Wu},
            year={2021},
            eprint={2108.06209},
            archivePrefix={arXiv},
            primaryClass={cs.LG}
      }</dd>

        <a href="#">&uarr; Back to top</a>
        </section>
    <section id="SoundStream">
        <h3>SoundSTream</h3>
        <dt>Paper name</dt><dd>Alpaca: A Strong, Replicable Instruction-Following Model</dd>
        <dt>Paper authors</dt><dd>Taori et al.</dd>
        <dt>Paper link</dt><dd><a href="https://crfm.stanford.edu/2023/03/13/alpaca.html">https://crfm.stanford.edu/2023/03/13/alpaca.html</a></dd>
        <dt>Publication Date</dt><dd>July 2021</dd>
        <dt>Repository link</dt><dd><a href="https://github.com/tatsu-lab/stanford_alpaca">https://github.com/tatsu-lab/stanford_alpaca</a></dd>
        <dt>Affiliation</dt><dd>Stanford</dd>
        <dt>Great Secondary Resources</dt><dd>SOMETHING</dd>

        <a href="#">&uarr; Back to top</a>
        </section>
    <section id="wav2vec">
        <h3>wav2vec</h3>
        <dt>Paper name</dt><dd>wav2vec: Unsupervised Pre-training for Speech Recognition</dd>
        <dt>Paper authors</dt><dd>Taori et al.</dd>
        <dt>Paper link</dt><dd><a href="https://arxiv.org/abs/1904.05862">https://arxiv.org/abs/1904.05862</a></dd>
        <dt>Publication Date</dt><dd>September 2019</dd>
        <dt>Repository link</dt><dd><a href="https://github.com/tatsu-lab/stanford_alpaca">https://github.com/tatsu-lab/stanford_alpaca</a></dd>
        <dt>Affiliation</dt><dd>Stanford</dd>
        <dt>Great Secondary Resources</dt><dd>SOMETHING</dd>
        <dt>BibTex Citation</dt><dd>@misc{schneider2019wav2vec,
            title={wav2vec: Unsupervised Pre-training for Speech Recognition}, 
            author={Steffen Schneider and Alexei Baevski and Ronan Collobert and Michael Auli},
            year={2019},
            eprint={1904.05862},
            archivePrefix={arXiv},
            primaryClass={cs.CL}
      }</dd>
        <a href="#">&uarr; Back to top</a>
        </section>
    <section id="BERT">
        <h3>BERT</h3>
        <dt>Paper name</dt><dd>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</dd>
        <dt>Paper authors</dt><dd>Taori et al.</dd>
        <dt>Paper link</dt><dd><a href="https://arxiv.org/abs/1810.04805">https://arxiv.org/abs/1810.04805</a></dd>
        <dt>Publication Date</dt><dd>October 2018</dd>
        <dt>Repository link</dt><dd><a href="https://github.com/tatsu-lab/stanford_alpaca">https://github.com/tatsu-lab/stanford_alpaca</a></dd>
        <dt>Affiliation</dt><dd>Stanford</dd>
        <dt>Great Secondary Resources</dt><dd>SOMETHING</dd>

        <a href="#">&uarr; Back to top</a>
        </section>
   </body>
</html>