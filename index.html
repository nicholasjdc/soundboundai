<!DOCTYPE html>
<html>
  <head>
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Hello!</title>
    <meta name="description" content="description"/>
    <meta name="author" content="author" />
    <meta name="keywords" content="keywords" />
    <link rel="stylesheet" href="./stylesheet.css" type="text/css" />
    <style type="text/css">.body { width: auto; }</style>
  </head>
  <body>
    </p>
    <a href="timeline.pdf">Download PDF</a>
    <div id="image">
        <p style="font-size: 80%;">This graph is clickable!</p>
        <img src="timeline.PNG"  width = 550 height = 350 alt="Timeline image" usemap="#timeline" />
    </div>
    <h2>Details</h2>
    <map id="timeline" name="timeline">
    <area shape="rect" id="node1" href="#MusicLM" title="MusicLM" alt="" coords="175,0,240,60"/>
    <area shape="rect" id="node2" href="#AudioLM" title="AudioLM" alt="" coords="175,80,240,110"/>
    <area shape="rect" id="node3" href="#MuLan" title="MuLan" alt="" coords="430,110,490,150"/>
    <area shape="rect" id="node4" href="#wav2vec-bert" title="wav2vec-BERT" alt="" coords="60,160,190,190"/>
    <area shape="rect" id="node5" href="#SoundStream" title="SoundStream" alt="" coords="300,200,400,230"/>
    <area shape="rect" id="node6" href="#wav2vec" title="wav2Vec" alt="" coords="60,250,150,280"/>
    <area shape="rect" id="node7" href="#BERT" title="BERT" alt="" coords="190,290,310,330"/>

    </map>
    <section id="MusicLM">
        <h3>MusicLM</h3>
        <dt>Paper name</dt><dd>MusicLM: Generating Music From Text</dd>
        <dt>Paper authors</dt><dd>Agostinelli et al.</dd>
        <dt>Paper link</dt><dd><a href="https://arxiv.org/abs/2301.11325">https://arxiv.org/abs/2301.11325</a></dd>
        <dt>Publication Date</dt><dd>January 2023</dd>
        <dt>Repository link</dt><dd><a href="https://github.com/lucidrains/musiclm-pytorch">https://github.com/lucidrains/musiclm-pytorch</a>
        <a href ="https://github.com/zhvng/open-musiclm">https://github.com/zhvng/open-musiclm</a>
        </dd>

        <dt>Affiliation</dt><dd>Google</dd>
        <dt>Great Secondary Resources</dt><dd><a href="https://google-research.github.io/seanet/musiclm/examples/">https://google-research.github.io/seanet/musiclm/examples/</a>
        <a href = "https://www.youtube.com/watch?v=2CUKU2iAzAs">https://www.youtube.com/watch?v=2CUKU2iAzAs</a>
        <a href = "musiclm_expl.html">musiclm_expl.html</a>

        </dd>
        <a href="#">&uarr; Back to top</a>
        </section>
    <section id="AudioLM">
        <h3>AudioLM</h3>
        <dt>Paper name</dt><dd>AudioLM: a Language Modeling Approach to Audio Generation</dd>
        <dt>Paper authors</dt><dd>Borsos et al.</dd>
        <dt>Paper link</dt><dd><a href="https://arxiv.org/abs/2209.03143">https://arxiv.org/abs/2209.03143</a></dd>
        <dt>Publication Date</dt><dd>September 2022</dd>
        <dt>Repository link</dt><dd><a href="https://github.com/tatsu-lab/stanford_alpaca">https://github.com/tatsu-lab/stanford_alpaca</a></dd>
        <dt>Affiliation</dt><dd>Google</dd>
        <dt>Great Secondary Resources</dt><dd><a href = "https://ai.googleblog.com/2022/10/audiolm-language-modeling-approach-to.html">https://ai.googleblog.com/2022/10/audiolm-language-modeling-approach-to.html</a>
        <a href = "https://www.shaped.ai/blog/sounding-the-secrets-of-audiolm">https://www.shaped.ai/blog/sounding-the-secrets-of-audiolm</a>
        <a href = "audiolm_expl.html">audiolm_expl.html</a>
        </dd>

        <a href="#">&uarr; Back to top</a>
        </section>
    <section id="MuLan">
        <h3>MuLan</h3>
        <dt>Paper name</dt><dd>MuLan: A Joint Embedding of Music Audio and Natural Language
        </dd>
        <dt>Paper authors</dt><dd>Huang et al.</dd>
        <dt>Paper link</dt><dd><a href="https://arxiv.org/abs/2208.12415">https://arxiv.org/abs/2208.12415</a></dd>
        <dt>Publication Date</dt><dd>August 2022</dd>
        <dt>Repository link</dt><dd><a href="https://github.com/lucidrains/musiclm-pytorch">https://github.com/lucidrains/musiclm-pytorch</a></dd>
        <dt>Affiliation</dt><dd>Google, Seoul National University</dd>
        <dt>Great Secondary Resources</dt><dd><a href = "mulan_expl.html">mulan_expl.html</a></dd>

        <a href="#">&uarr; Back to top</a>
        </section>
    <section id="wav2vec-bert">
        <h3>Wav2Vec-BERT</h3>
        <dt>Paper name</dt><dd>W2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training</dd>
        <dt>Paper authors</dt><dd>Chung et al.</dd>
        <dt>Paper link</dt><dd><a href="https://arxiv.org/abs/2108.06209">https://arxiv.org/abs/2108.06209</a></dd>
        <dt>Publication Date</dt><dd>August 2021</dd>
        <dt>Repository link</dt><dd><a href="https://github.com/aispeech-lab/w2v-cif-bert">https://github.com/aispeech-lab/w2v-cif-bert</a></dd>
        <dt>Affiliation</dt><dd>MIT, Google</dd>
        <dt>Great Secondary Resources</dt><dd><a href = "https://qudata.com/en/news/w2v-bert/">https://qudata.com/en/news/w2v-bert/</a> <a href = "w2v-bert_expl">https://qudata.com/en/news/w2v-bert/</a></dd>
        <dt>Bib Tex Citation</dt><dd>@misc{chung2021w2vbert,
            title={W2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training}, 
            author={Yu-An Chung and Yu Zhang and Wei Han and Chung-Cheng Chiu and James Qin and Ruoming Pang and Yonghui Wu},
            year={2021},
            eprint={2108.06209},
            archivePrefix={arXiv},
            primaryClass={cs.LG}
      }</dd>

        <a href="#">&uarr; Back to top</a>
        </section>
    <section id="SoundStream">
        <h3>SoundSTream</h3>
        <dt>Paper name</dt><dd>SoundStream: An End-to-End Neural Audio Codec
        </dd>
        <dt>Paper authors</dt><dd>Zeghidour et al.</dd>
        <dt>Paper link</dt><dd><a href="https://arxiv.org/abs/2107.03312">https://arxiv.org/abs/2107.03312</a></dd>
        <dt>Publication Date</dt><dd>July 2021</dd>
        <dt>Repository link</dt><dd><a href="https://github.com/tatsu-lab/stanford_alpaca">https://github.com/tatsu-lab/stanford_alpaca</a></dd>
        <dt>Affiliation</dt><dd>Google</dd>
        <dt>Great Secondary Resources</dt><dd><a href = "https://ai.googleblog.com/2021/08/soundstream-end-to-end-neural-audio.html">https://ai.googleblog.com/2021/08/soundstream-end-to-end-neural-audio.html</a>
        <a href = "w2v_expl.html">w2v_expl.html</a>
        </dd>

        <a href="#">&uarr; Back to top</a>
        </section>
    <section id="wav2vec">
        <h3>wav2vec</h3>
        <dt>Paper name</dt><dd>wav2vec: Unsupervised Pre-training for Speech Recognition</dd>
        <dt>Paper authors</dt><dd>Schneider et al.</dd>
        <dt>Paper link</dt><dd><a href="https://arxiv.org/abs/1904.05862">https://arxiv.org/abs/1904.05862</a></dd>
        <dt>Publication Date</dt><dd>September 2019</dd>
        <dt>Repository link</dt><dd><a href="https://github.com/tatsu-lab/stanford_alpaca">https://github.com/tatsu-lab/stanford_alpaca</a></dd>
        <dt>Affiliation</dt><dd>Facebook</dd>
        <dt>Great Secondary Resources</dt><dd><a href = "https://maelfabien.github.io/machinelearning/wav2vec/#b-the-model">https://maelfabien.github.io/machinelearning/wav2vec/#b-the-model</a>
        <a href = "w2v_expl.html">w2v_expl.html</a>
        </dd>
        <dt>BibTex Citation</dt><dd>@misc{schneider2019wav2vec,
            title={wav2vec: Unsupervised Pre-training for Speech Recognition}, 
            author={Steffen Schneider and Alexei Baevski and Ronan Collobert and Michael Auli},
            year={2019},
            eprint={1904.05862},
            archivePrefix={arXiv},
            primaryClass={cs.CL}
      }</dd>
        <a href="#">&uarr; Back to top</a>
        </section>
    <section id="BERT">
        <h3>BERT</h3>
        <dt>Paper name</dt><dd>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</dd>
        <dt>Paper authors</dt><dd>Devlin et al.</dd>
        <dt>Paper link</dt><dd><a href="https://arxiv.org/abs/1810.04805">https://arxiv.org/abs/1810.04805</a></dd>
        <dt>Publication Date</dt><dd>October 2018</dd>
        <dt>Repository link</dt><dd><a href="https://www.kaggle.com/code/harshjain123/bert-for-everyone-tutorial-implementation">https://www.kaggle.com/code/harshjain123/bert-for-everyone-tutorial-implementation</a></dd>
        <dt>Affiliation</dt><dd>Google</dd>
        <dt>Great Secondary Resources</dt><dd>
            <a href = "https://neptune.ai/blog/how-to-code-bert-using-pytorch-tutorial">https://neptune.ai/blog/how-to-code-bert-using-pytorch-tutorial</a>
            <a href = "BERT_expl.html">BERT_expl.html</a>
        </dd>

        <a href="#">&uarr; Back to top</a>
        </section>
   </body>
</html>